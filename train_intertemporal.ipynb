{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"train_combined_pub_0313_0905.csv\"\n",
    "df_train_raw = read_data(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['MAXEOHSTATEOFCHARGE','PRODUCTBID_DESC','PRODUCTBID_MRID', 'MARKETPRODUCT_DESC', 'SCH_BID_Y2AXISDATA', 'MINEOHSTATEOFCHARGE', 'MAXEOHSTATEOFCHARGE', 'STARTTIME', 'STOPTIME', 'RESOURCE_TYPE', 'TIMEINTERVALSTART', 'TIMEINTERVALEND', 'MARKETPRODUCTTYPE', 'SCH_BID_TIMEINTERVALSTART_GMT', 'SCH_BID_TIMEINTERVALSTOP_GMT', 'SCH_BID_CURVETYPE']\n",
    "df_train_droped = df_train_raw.drop(columns=columns_to_drop, axis=\"columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESOURCEBID_SEQ = 100651 #None\n",
    "RESOURCEBID_SEQ = 117712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "size of data is: (0, 9)\n",
      "=====================================================\n",
      "Statistics in data:\n",
      "        SCHEDULINGCOORDINATOR_SEQ  RESOURCEBID_SEQ  SELFSCHEDMW  \\\n",
      "count                        0.0              0.0          0.0   \n",
      "mean                         NaN              NaN          NaN   \n",
      "min                          NaN              NaN          NaN   \n",
      "25%                          NaN              NaN          NaN   \n",
      "50%                          NaN              NaN          NaN   \n",
      "75%                          NaN              NaN          NaN   \n",
      "max                          NaN              NaN          NaN   \n",
      "std                          NaN              NaN          NaN   \n",
      "\n",
      "      SCH_BID_TIMEINTERVALSTART SCH_BID_TIMEINTERVALSTOP  SCH_BID_XAXISDATA  \\\n",
      "count                         0                        0                0.0   \n",
      "mean                        NaT                      NaT                NaN   \n",
      "min                         NaT                      NaT                NaN   \n",
      "25%                         NaT                      NaT                NaN   \n",
      "50%                         NaT                      NaT                NaN   \n",
      "75%                         NaT                      NaT                NaN   \n",
      "max                         NaT                      NaT                NaN   \n",
      "std                         NaN                      NaN                NaN   \n",
      "\n",
      "       SCH_BID_Y1AXISDATA  hr_start  hr_stop  \n",
      "count                 0.0       0.0      0.0  \n",
      "mean                  NaN       NaN      NaN  \n",
      "min                   NaN       NaN      NaN  \n",
      "25%                   NaN       NaN      NaN  \n",
      "50%                   NaN       NaN      NaN  \n",
      "75%                   NaN       NaN      NaN  \n",
      "max                   NaN       NaN      NaN  \n",
      "std                   NaN       NaN      NaN  \n",
      "=====================================================\n",
      "Top few rows in data:\n",
      "Empty DataFrame\n",
      "Columns: [SCHEDULINGCOORDINATOR_SEQ, RESOURCEBID_SEQ, SELFSCHEDMW, SCH_BID_TIMEINTERVALSTART, SCH_BID_TIMEINTERVALSTOP, SCH_BID_XAXISDATA, SCH_BID_Y1AXISDATA, hr_start, hr_stop]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "RESOURCEBID_SEQ = 117712\n",
    "df_train_filtered_test = filter_rows(df_train_droped, RESOURCEBID_SEQ)\n",
    "printStats(df_train_filtered_test, type=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m df_train_filtered \u001b[38;5;241m=\u001b[39m filter_rows(df_train_droped, HOUR, RESOURCEBID_SEQ)\n\u001b[1;32m     23\u001b[0m df_train_feat \u001b[38;5;241m=\u001b[39m extract_feat_from_bid(df_train_filtered)\n\u001b[0;32m---> 24\u001b[0m df_train_feat_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscale_feat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_feat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m df_train_feat_scaled_pca, pca_components_train, explained_variance_train, pca_train \u001b[38;5;241m=\u001b[39m extract_pca_components(df_train_feat_scaled, goal_var\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m)\n\u001b[1;32m     26\u001b[0m list_pca_train\u001b[38;5;241m.\u001b[39mappend(pca_train)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofIllinois-Urbana/24Sp/ENG573-SIEMENS/Codes/Capstone_Project/utils.py:211\u001b[0m, in \u001b[0;36mscale_feat\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m    210\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m--> 211\u001b[0m features_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m df_scaled \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(features_scaled, index\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_scaled\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:876\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:912\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[1;32m    881\u001b[0m \n\u001b[1;32m    882\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    911\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 912\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    875\u001b[0m pandas_requires_conversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    876\u001b[0m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[1;32m    877\u001b[0m )\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[0;32m--> 879\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdtypes_orig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pandas_requires_conversion \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(d \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;66;03m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[1;32m    882\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mresult_type\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from draw_bid_curve import find_trace_bid_curves_for_multiple_days_2d\n",
    "\n",
    "# create a subplot grid: 4 rows, 6 columns\n",
    "rows = 4\n",
    "cols = 6\n",
    "fig_err = make_subplots(rows=rows, cols=cols, subplot_titles=[f\"Hour {h}\" for h in range(24)], horizontal_spacing=0.03, vertical_spacing=0.05)\n",
    "fig_bid_train = make_subplots(rows=rows, cols=cols, subplot_titles=[f\"Hour {h}\" for h in range(24)], horizontal_spacing=0.03, vertical_spacing=0.05)\n",
    "\n",
    "fig_clustered_bid_train = make_subplots(rows=rows, cols=cols, subplot_titles=[f\"Hour {h}\" for h in range(24)], horizontal_spacing=0.03, vertical_spacing=0.05)\n",
    "\n",
    "min_clusters=2\n",
    "max_clusters= 10 #20\n",
    "\n",
    "best_clusters_by_hour=[]\n",
    "list_pca_train = []\n",
    "list_model_kmeans_train = []\n",
    "list_cluster_thresholds = []\n",
    "for HOUR in range(24):\n",
    "    # print(f\"Processing Hour {HOUR}...\")\n",
    "    df_train_filtered = filter_rows(df_train_droped, HOUR, RESOURCEBID_SEQ)\n",
    "    df_train_feat = extract_feat_from_bid(df_train_filtered)\n",
    "    df_train_feat_scaled = scale_feat(df_train_feat)\n",
    "    df_train_feat_scaled_pca, pca_components_train, explained_variance_train, pca_train = extract_pca_components(df_train_feat_scaled, goal_var=0.95)\n",
    "    list_pca_train.append(pca_train)\n",
    "    \n",
    "    model_kmeans_train, n_cluster = find_best_model(df_train_feat_scaled_pca, min_clusters=min_clusters, max_clusters=max_clusters)\n",
    "    cluster_thresholds = find_cluster_threshold(model_kmeans_train, df_train_feat_scaled_pca, n_cluster)\n",
    "    list_cluster_thresholds.append(cluster_thresholds)\n",
    "    \n",
    "    best_clusters_by_hour.append(n_cluster)\n",
    "    list_model_kmeans_train.append(model_kmeans_train)\n",
    "    \n",
    "    df_train_feat_scaled_pca_w_label = df_train_feat_scaled_pca.copy()\n",
    "    df_train_feat_scaled_pca_w_label['cluster_label'] = model_kmeans_train.labels_\n",
    "    cluster_labels = df_train_feat_scaled_pca_w_label[['cluster_label']].copy()\n",
    "    \n",
    "    row = (HOUR // cols) + 1\n",
    "    col = (HOUR % cols) + 1   \n",
    "    \n",
    "    showlegend=True if HOUR == 0 else False\n",
    "    kwargs = {'showlegend': showlegend}\n",
    "    \n",
    "    traces_err = find_trace_inter_intra_err(df_train_feat_scaled_pca, min_clusters, max_clusters, **kwargs)\n",
    "    for trace in traces_err:\n",
    "        fig_err.add_trace(trace, row=row, col=col)\n",
    "    \n",
    "    traces_bid_train = find_trace_bid_curves_for_multiple_days_2d(df_train_filtered, HOUR)['traces']\n",
    "    for trace in traces_bid_train:\n",
    "        fig_bid_train.add_trace(trace, row=row, col=col)\n",
    "    \n",
    "    traces_clustered_bid_train = find_trace_bid_curves_for_multiple_days_2d(df_train_filtered, HOUR, cluster_labels)['traces']\n",
    "    for trace in traces_clustered_bid_train:\n",
    "        fig_clustered_bid_train.add_trace(trace, row=row, col=col)\n",
    "        \n",
    "\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x=list(range(24)) ,\n",
    "    y=best_clusters_by_hour,\n",
    "    mode='lines+markers',  # Both lines and markers\n",
    "    marker=dict(size=8, color='blue'),  # Marker settings\n",
    "    line=dict(color='blue')  # Line settings\n",
    ")\n",
    "fig_best_clusters = go.Figure(data=[trace])\n",
    "fig_best_clusters.update_layout(\n",
    "    title='Best number of clusters by hour (for training set)',\n",
    "    xaxis_title='Hour',\n",
    "    yaxis_title='Number of custers',\n",
    "    height=800,  # height of the figure in pixels\n",
    "    width=800,   # width of the figure in pixels\n",
    "    hovermode=\"x unified\", \n",
    ")\n",
    "\n",
    "    \n",
    "fig_height_factor = 250\n",
    "fig_width_factor = fig_height_factor * 1.25 \n",
    "\n",
    "fig_bid_train.update_layout(\n",
    "    height=rows*fig_height_factor,\n",
    "    width=cols*fig_width_factor,\n",
    "    title_text=\"Bidding curves by hour (X axis: Amount (MW), Y axis: Price ($)) (for training set)\",\n",
    "    showlegend=False,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        # y=1.05,\n",
    "        xanchor=\"right\",\n",
    "        # x=1\n",
    "    ),\n",
    "    hovermode=\"x unified\", \n",
    ")\n",
    "\n",
    "      \n",
    "fig_clustered_bid_train.update_layout(\n",
    "    height=rows*fig_height_factor,\n",
    "    width=cols*fig_width_factor,\n",
    "    title_text=\"Clustered bidding curves by hour (X axis: Amount (MW), Y axis: Price ($)) (for training set)\",\n",
    "    showlegend=False,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        # y=1.05,\n",
    "        xanchor=\"right\",\n",
    "        # x=1\n",
    "    ),\n",
    "    hovermode=\"x unified\", \n",
    ")\n",
    "   \n",
    "fig_err.update_layout(\n",
    "    height=rows*fig_height_factor,\n",
    "    width=cols*fig_width_factor,\n",
    "    title_text=\"Intercluster and intracluster errors by hour (X axis: Num of clusters, Y axis: Error(Sum of sqared distances)) (for training set)\",\n",
    "    legend=dict(\n",
    "        orientation=\"h\", #horizontal legend\n",
    "        yanchor=\"top\",\n",
    "        y=1.05,\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    ),\n",
    "    hovermode=\"x unified\", \n",
    ")\n",
    "\n",
    "# Save the figure\n",
    "fig_bid_train.write_html(\"output/train_bid.html\")\n",
    "fig_err.write_html(\"output/train_intra_inter_err.html\")\n",
    "fig_best_clusters.write_html(\"output/train_best_clusters.html\")\n",
    "fig_clustered_bid_train.write_html(\"output/train_clustered_bid.html\")\n",
    "\n",
    "# Show the figure\n",
    "# fig_bid_train.show()\n",
    "# fig_err.show()\n",
    "# fig_best_clusters.show()    \n",
    "# fig_clustered_bid_train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./img/percentile_cluster1.jpg\" width=\"60%\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_file = \"valid_combined_pub_0906_1023.csv\"\n",
    "df_valid_raw = read_data(valid_file)\n",
    "# printStats(df_valid_raw)\n",
    "\n",
    "df_valid_droped = df_valid_raw.drop(columns=columns_to_drop, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_bid_valid = make_subplots(rows=rows, cols=cols, subplot_titles=[f\"Hour {h}\" for h in range(24)], horizontal_spacing=0.03, vertical_spacing=0.05)\n",
    "\n",
    "fig_clustered_bid_valid = make_subplots(rows=rows, cols=cols, subplot_titles=[f\"Hour {h}\" for h in range(24)], horizontal_spacing=0.03, vertical_spacing=0.05)\n",
    "\n",
    "\n",
    "for HOUR in range(24):\n",
    "    df_valid_filtered = filter_rows(df_valid_droped, HOUR, RESOURCEBID_SEQ)\n",
    "    df_valid_feat = extract_feat_from_bid(df_valid_filtered)\n",
    "    df_valid_feat_scaled = scale_feat(df_valid_feat)\n",
    "    df_valid_feat_scaled_pca = convert_numpy_pca_to_df(list_pca_train[HOUR].transform(df_valid_feat_scaled), df_valid_feat_scaled)\n",
    "\n",
    "    # valid_labels = list_model_kmeans_train[HOUR].predict(df_valid_feat_scaled_pca)\n",
    "    cluster_thresholds = list_cluster_thresholds[HOUR]\n",
    "    valid_labels = predict_valid_label(list_model_kmeans_train[HOUR], df_valid_feat_scaled_pca, cluster_thresholds)\n",
    "    df_valid_feat_scaled_pca_w_label = df_valid_feat_scaled_pca.copy()\n",
    "    df_valid_feat_scaled_pca_w_label['cluster_label'] = valid_labels\n",
    "\n",
    "    cluster_labels_valid = df_valid_feat_scaled_pca_w_label[['cluster_label']].copy()\n",
    "\n",
    "    row = (HOUR // cols) +  1\n",
    "    col = (HOUR % cols) + 1\n",
    "    showlegend=True if HOUR == 0 else False\n",
    "    kwargs = {'showlegend': showlegend}\n",
    "    \n",
    "    traces_bid_valid = find_trace_bid_curves_for_multiple_days_2d(df_valid_filtered, HOUR)['traces']\n",
    "    for trace in traces_bid_valid:\n",
    "        fig_bid_valid.add_trace(trace, row=row, col=col)\n",
    "    \n",
    "    traces_clustered_bid_valid = find_trace_bid_curves_for_multiple_days_2d(df_valid_filtered, HOUR, cluster_labels_valid)['traces']\n",
    "    for trace in traces_clustered_bid_valid:\n",
    "        fig_clustered_bid_valid.add_trace(trace, row=row, col=col)\n",
    "\n",
    "fig_bid_valid.update_layout(\n",
    "    height=rows*fig_height_factor,\n",
    "    width=cols*fig_width_factor,\n",
    "    title_text=\"Bidding curves by hour (X axis: Amount (MW), Y axis: Price ($)) (for testing(or validation) set)\",\n",
    "    showlegend=False,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        # y=1.05,\n",
    "        xanchor=\"right\",\n",
    "        # x=1\n",
    "    ),\n",
    "    hovermode=\"x unified\", \n",
    ")\n",
    "      \n",
    "fig_clustered_bid_valid.update_layout(\n",
    "    height=rows*fig_height_factor,\n",
    "    width=cols*fig_width_factor,\n",
    "    title_text=\"Clustered bidding curves by hour (X axis: Amount (MW), Y axis: Price ($)) (for testing(or validation) set)\",\n",
    "    showlegend=False,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        # y=1.05,\n",
    "        xanchor=\"right\",\n",
    "        # x=1\n",
    "    ),\n",
    "    hovermode=\"x unified\", \n",
    ")\n",
    "\n",
    "# Save the figure\n",
    "fig_bid_valid.write_html(\"output/valid_bid.html\")\n",
    "fig_clustered_bid_valid.write_html(\"output/valid_clustered_bid.html\")\n",
    "\n",
    "# Show the figure\n",
    "# fig_bid_valid.show()\n",
    "# fig_clustered_bid_valid.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
